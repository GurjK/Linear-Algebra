While the first 3 matrices listed below aren't necessarily special forms or matrices with special properties they are common techniques or operations applied to matrices or propeties of matrices that are critical to understand:
- Transpose: $A^T$
- Inverse : $A^{-1}$
- Determinant : $det(A)$

## Transposed Matrices

The transpose of a matrix is very similar to the transpose of a matrix and logically it means essentially flipping a matrices dimensions to the opposite coordinate

To Transpose a matrix M = n x m  
- $M^T$ = m x n  
- The original elements of the $a_{i,j}$ of M would become $a_{j,i}$ in $M^{T}$

### Properties of a Transposed Matrix
> -  $(A^{T){^T}}= A$  
> -  $(A+B)^{T} = A^{T} +B^{T}$  
> -  $(Av)^{T}=v^TA^T$  
> -  $(AB)^T=B^TA^T$  

---

## Inverse Matrices

The inverse of a matrix can be thought of similarly to the inverse of a function. If we have a function $f(x) = y$. The function that maps $y$ to $x$ such that $f(x) = y$ is called the inverse of $f$.

so $f(f^{-1}(x))=y$

Now if we are to look at the formula $Ax = b$, a OLS system, in order to solve for $x$ we should have to shift $A$ to the other side giving us $x=A^{-1}b$.

$A^{-1}$ is know as the inverse of the Matrix $A$. The matrix $A$ does not necessarily have an inverse but for it to have an inverse it must satisfy the following:

- A has to be a square matrix  
- A must have a non zero determinant ($det(A)\neq 0$)

If this holds true then the matrix $A$ is also said to be a **non-singular** matrix but if it fails either condition the $A$ is **singular**

To summarize if $L$ is a linear transformation, $A$ is a matrix that represents $L$. There exists a matrix $B$ such that $AB = I$, then $L$ has an inverse $L^{-1}$ and $B$ represents that Linear Transformation ($L^{-1}$)

### Properties of an Inverse Matrix
> - $AA^{-1}=A^{-1}A=I$  
> - $(A^T)^{-1}=(A^{-1})^{T}$  
> - If $AA=I$ then $A=A^{-1}$  
> - $I^{-1}=I$  
> - If $A^{-1}$ exists, then $A$ is non-singular  
> - If $\det(A) \neq 0$, then $A$ is non-singular, so the inverse exists  
> - $(AB)^{-1} = B^{-1}A^{-1}$  
> - $(\alpha B)^{-1} = \frac{1}{\alpha}B^{-1}$  
> - $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$  
> - $(A^{-1})^{-1} = A$

Inversing a matrix by hand isn't ideal and is computationally difficult for matrices as they grow. For example for a general matrix the inverse is as follows:

$$
A^{-1} = \frac{1}{det(A)} * adj(A)
$$

where the $adj(A)$ is the transpose of the cofactor matrix - The cofactor of an element $a_{i,j}$ is the determinant of the submatrix obtained by removing the $i^{th}$ row and $j^{th}$ column, multiplied by $(-1)^{i+j}$

For a 2 x 2 matrix it it can be simplified to this:

$$
A^{-1} = \frac{1}{det(A)} * \begin{bmatrix} a & -b \\ 
-c & d \end{bmatrix}
$$

As matrices grow this is computationally very inefficient and alternative methods are presented to avoid doing this directly. Some of the issues are as follows:

- Numerical Instability  
  - Due to the number of floating point calculations needed to inverse a matrix a small determinant or very large determinant can lead to inaccurate results  
- Ill Conditioned Matrix  
  - The **condition number** of a matrix is $\text{cond}(A) = \| A \| \cdot \| A^{-1} \|$  
  - A high condition number (> $10^{6}$ ) indicates ill conditioned  
  - Even if the determinant is not zero but it has a high condition number it means that the results of the inversion may not be reliable so any sort of statistics calculated cannot be trusted completely  
- Computationally expensive  
  - Inverting a matrix is very expensive in terms of memory and floating point operations  

There are multiple methods to avoid computing the inverse directed as in the case of an OLS problem. Some of the methods are listed below.

- Numerical Methods  
  - LU Decompostition  
  - QR Decomposition  
  - Cholesky Decomposition  
- Iterative Solvers  
  - Jacobi  
  - Gauss Seidel Methods  
- Regularization  
  - If A is ill conditioned add a regularization term to improve stability $A_{reg} = A + \lambda I$

---

## Determinant of a Matrix

The determinant has the same intuitive meaning but it depends on the size of the matrix

For a 2x2 matrix, it gives the area of parallelogram form by the linear transformation presented for the matrix. For a 3x3 matrix, it gives the volume of the parallelepiped formed by the column vectors. For higher dimensions, it gives the hypervolume of the n dimensional parallelepiped formed by the column vectors.

If the determinant is 0, it reduced the area/volume to 0 reducing the space to a lower dimension. A positive determinant ensures the orientation is the same, while a negative determinant leads to a reflection.

### Properties of Determinants
> - If $det(A) = 0$  
>   - matrix is **singular**  
>   - columns are linearly dependent  
>   - No unique solution to $Ax=b$  
> - If $det(A) \neq 0$  
>   - non-singular  
>   - matrix is invertible.  
>   - columns are linearly independent.  
>   - Unique Solution to $Ax=b$  
> - $det(A^T)=det(A)$  
> - $det(D) = $ product of all main diagonal entries  
> - $det(I) = 1$  
> - $det(L) = $ product of all main diagonal entries  
> - $det(U) = $ product of all main diagonal entries  
> - $det(AB) = det(A) det(B)$  
> - $det(A^T) = det(A)$  

For a (2 x2) matrix:

$$
A = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$

The determinant is:

$$
\text{det}(A) = ad - bc
$$

For a (3 x 3) matrix:

$$
A = \begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
$$

The determinant is:

$$
\text{det}(A) = a(ei - fh) - b(di - fg) + c(dh - eg)
$$

---

## Symmetric Matrices

A symmetric matrix is defined as follows:  
- $A_{j,k} = A^{T}_{k,j}$  
- essentially can be viewed as a mirror image of the matrix over the diagonal of the matrix

### Properties of a Symmetric Matrix
> - If $A = B^T$, it does mean that $AB = (AB)^T$ and $AB$ is symmetric.  
> - If $A$ is symmetric, then:  
>   - $A = A^T$  
> - If $A$ and $B$ are symmetric, then:  
>   - $A + B$ is symmetric.  
>   - $AB$ is symmetric if $AB = BA$.

A symmetric matrix $S$ is defined as:

$$
S = \begin{bmatrix}
s_{11} & s_{12} & s_{13} & \cdots & s_{1n} \\
s_{12} & s_{22} & s_{23} & \cdots & s_{2n} \\
s_{13} & s_{23} & s_{33} & \cdots & s_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
s_{1n} & s_{2n} & s_{3n} & \cdots & s_{nn}
\end{bmatrix}
$$

---

## Square Matrix

A square matrix a a matrix that has the same number of rows and columns so  
- S = m x m where m = # of rows and columns

---

## Identity Matrices

An identity matrix is defined typically by the symbol $I$ and as follows:
- A Square matrix with 1s on the diagonal and 0s everywhere else
- Each columns of the identity matrix represents the standard unit basis vector
- If you multiply a Matrix by a specific column in the identity matrix then you just get that corresponding column back
- Multiply a Matrix by the Identity matrix returns that Matrix

> ### Properties of the Identity Matrix
> - $AI = IA = A$  
> - $I^{-1}=I$  
> - $det(I_n)=1$  
> - $I^k=I$ for an $k\geq 1$  
> - $Tr(I_n) = n$  
> - $\lambda =1$

The identity matrix $I$ is defined as:

$$
I = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$

---

## Triangular Matrices

A triangular matrix is a matrix where either all the elements above the diagonal or below the diagonal are all 0. This results in either a Lower Triangular Matrix or Upper Triangular Matrix.

### Properties of Triangular Matrices
> - $L^T=U$ and $U^T=L$  
> - LL = L and UU=U  
> - $LL^{-1}=I$  
> - $UU^{-1}=I$  
> - $det(L) = $ product of all main diagonal entries  
> - $det(U) = $ product of all main diagonal entries  
> - L is nonsingular and therefore invertible iff all entries on main diagonal are non-zero  
> - U is nonsingular and therefore invertible iff all entries on main diagonal are non-zero

A lower triangular matrix $L$ is defined as:

$$
L = \begin{bmatrix}
l_{11} & 0 & 0 & \cdots & 0 \\
l_{21} & l_{22} & 0 & \cdots & 0 \\
l_{31} & l_{32} & l_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & l_{n3} & \cdots & l_{nn}
\end{bmatrix}
$$

An upper triangular matrix $U$ is defined as:

$$
U = \begin{bmatrix}
u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
0 & u_{22} & u_{23} & \cdots & u_{2n} \\
0 & 0 & u_{33} & \cdots & u_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & u_{nn}
\end{bmatrix}
$$

---

## Diagonal Matrices

A diagonal matrix, D, is a special type of symmetric matrix where every element not on the main diagonal is 0. A diagonal matrix has the following characteristics:
- symmetric  
- sum and product of 2 D is D  
- D is both U and L  

A diagonal matrix is allowed to have 0 on the main diagonal.

### Properties of a Diagonal Matrix
> - $det(D)$ = product of its diagonal entries  
> - Diagonal entries are the eigenvalues of the matrix  
> - $D^{-1}$ = take each $d_{i,j}$ and turn it to $\frac{1}{d_{i,j}}$  
> - $D^k$ = take each $d_{i,j}$ and turn it to $d_{i,j}^{k}$  
> - AD = Scale each column of A by corresponding diagonal entry in D  
> - Non-Singular iff each $d_{i,j} \neq 0$

A diagonal matrix $D$ is defined as:

$$
D = \begin{bmatrix}
d_{11} & 0 & 0 & \cdots & 0 \\
0 & d_{22} & 0 & \cdots & 0 \\
0 & 0 & d_{33} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & d_{nn}
\end{bmatrix}
$$

---

## Non-Singular and Singular Matrices

A non- singular matrix and singular matrix have unique properties that make them important for linear algebra. The main deterministic features of them are the ability to invert them and the determinant of a matrix. They must be a **square** matrix!

### Non-Singular Matrix
> - $det(A) \neq 0$  
> - $A^{-1}$ exists  
> - Rows and columns are linearly independent so it is full rank=n  
> - $Ax=b$ has a unique solution  
> - Range(A) = $R^{n}$  
> - rank(A) = n  
> - Null(A) = 0

### Singular Matrix
> - $det(A) = 0$  
> - $A^{-1}$ does not exist  
> - Rank is less than n so it does not span the full n-dimensional space  
> - Linearly dependent columns  
> - $Ax=b$ has no solution or infinitely many solutions










