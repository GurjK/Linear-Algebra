
One of the issues that comes from LU Decomposition, is its ability to immediately ruin good properties presented in a possibly symmetric or diagonal matrix. The first step of a LU decomposition involves possible row swapping or column swapping depending on the algorithm presented so it isn't an ideal algorithm for these situations. 

## Diagonal Dominance

There are certain cases where LU Decomposition can be performed without pivoting, and one of those situation are where the diagonals of the matrix, A is greater then the sum of the rest of entries in that row. This is due to the fact that the diagonal entries dominate the other entires. This is called **row diagonally dominant**

$$
 |(a_ii)| \geq \sum_{j=1,j \neq i,i}^n |a_{ij}|
$$
**Column Diagonally Dominant** would be the opposite case where the diagonally element is greater then the rest of the elements in that column.

If the matrix is Strongly diagonally dominant ( e.g.  > not $\geq$) then it is **non-singular** but if it is weakly diagonally dominant (e.g. $\geq$ not >) then it may still be singular. 

In Cases where A is strongly diagonally dominant then LU Decomposition without Pivoting is a stable and acceptable algorithm.

## Symmetric Matrices: $LDL^T$

If we have a matrix A that is symmetric then its LU decomposition can be broken down as follows:

$$
A = \begin{bmatrix}
a & c \\
c & d
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 \\
\frac{c}{a} & 1
\end{bmatrix}
\begin{bmatrix}
a & c \\
0 & d-(\frac{c}{a})c
\end{bmatrix}
$$
We can see that U is can be replaced as a transpose of L and a diagonal matrix D

$$
A = \begin{bmatrix}
a & c \\
c & d
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 \\
\frac{c}{a} & 1
\end{bmatrix}
\begin{bmatrix}
a & c \\
0 & d-(\frac{c}{a})c
\end{bmatrix}
$$


$$
A = \begin{bmatrix}
a & c \\
c & d
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 \\
\frac{c}{a} & 1
\end{bmatrix}
\begin{bmatrix}
a & 0 \\
0 & d-(\frac{c}{a})c
\end{bmatrix}
\begin{bmatrix}
1 & \frac{c}{a} \\
0 & 1
\end{bmatrix}
$$
We can now see and conclude that :

if A is symmetric and the principal submatrix is nonsingular then there exists a unit lower triangular matrix L, and diagonal matrix D such that

$$
A =LDL^T
$$
This matrix requires roughly half the number of floating point operations as Gaussian elimination. 

If the matrix is ill conditioned then we can use the form of  a permutation matrix using symmetric pivoting such that 
$$
PAP^T = LDL^T
$$

## Positive Definite Matrices

A matrix is said to be **Positive Definite** if 
$$
x^TAx > 0
$$
for any vector x $\in$ $R^n$. 

- A positive definite matrix is nonsingular by default 
- all the entries on the diagonal must be positive
- The matrix is positive definite iff the symmetric matrix $T =\frac{A +A^T}{2}$ has positive eigenvalues
- if A is positive definite then it has an LU factorization and the diagonal entries of U are all positive
- The inverse of a symmetric positive definite matrix is also a symmetric positive definite matrix
- How to check if something is symmetric positive definite
	- Check if diagonals are > 0
	- Check if strictly diagonally dominant

A matrix is **Semi-Positive Definite** if
$$
x^TAx \geq 0
$$
for any vector x $\in$ $R^n$

$$
A = \begin{bmatrix}
a & c \\
c & d
\end{bmatrix}
$$
 In this matrix it is symmetric positive definite if there is an entry on the diagonal such that it is big enough in magnitude so that there is no need for pivoting. A good algorithm to run for this could be the Cholseky.

## Symmetric Positive Definite: Cholesky $GG^T$

So far we know the following:

Symmetric Matrix: $A=LDL^T$
Symmetric Positive Definite $A=LU$

Another exploitation for a Symmetric Positive Definite Matrix is $A=GG^T$, where G is really a lower triangular matrix. and this is done through the Cholesky factorization. 

### Solving Ax=b through Cholesky

1. Find Cholesky (Shown later) =G
2. Ax=b
3. $GG^Tx=b$
4. $Gy=b$
5. $G^Tx=y$

## Cholesky Algorithm
1. For each column j:
	1. Compute the diagonal entry $G(j,j)$-Step 2
	2. Compute the entries below $G(j,j)$-Step 3
2. Compute the Diagonal Entry
	1. $G(j,j) = \sqrt{A(j,j)}$
3. Compute the entries below $G(j,j)$
	1. For i = j+1 to:
		1. $A(i,j) \to A(i,j) -\sum_{k=1}^{j-1} G(i,k)*G(j,k)$
		2. $G(i,j) =\frac{A(i,j)}{G(j,j)}$
4. Update remaining entries of A 
	1. $A(i:n,j:n) \to A(i:n,j:n)-G(i:n,j)*G(j,j)^T$
5. Repeat for each column, ending with the result G

#### Example of the Cholesky Algorithm

$$
A = \begin{bmatrix}
4 & 2 & 4 \\
2 & 10 & 5 \\
4 & 5 & 9
\end{bmatrix}
$$

For **Column 1**: 
- $G(j,j) = \sqrt{A(j,j)}=G(1,1)=\sqrt{A(4)}=2$
- $A(i+1,1)=A(2,1)=2,A(i+2,1)=A(3,1)=4$ : Nothing to subtract above it
-  $G(i,j)=G(2,1)=\frac{2}{2}=1,G(i,j)=G(3,1)=\frac{4}{2}=2$
$$
G=\begin{bmatrix}
2 & 0 & 0 \\
1 & 0 & 0 \\
2 & 0 & 0
\end{bmatrix}
$$
- $A(2:3,2:3) = A(2:3,2:3)-G(2:3,1)*G(2:3,1)^T$ 

$$
A(2:3,2:3)= \begin{bmatrix}
10 & 5 \\
5 & 9 
\end{bmatrix} - \begin{bmatrix} 1 \\ 2 \end{bmatrix}\begin{bmatrix} 1 & 2 \end{bmatrix} =
\begin{bmatrix}
9 & 3 \\
3 & 5
\end{bmatrix}
$$
 $$A = \begin{bmatrix}
   4 & 2 & 4 \\
   2 & 9 & 3 \\
   4 & 3 & 2
   \end{bmatrix}.$$

For **Column 2**:

- $G(2, 2) = \sqrt{A(2, 2)} = \sqrt{9} = 3$

- $A(3, 2) = A(3, 2) - G(3, 1) \cdot G(2, 1)$:
  $A(3, 2) = 3 - (2 \cdot 1) = 3$
  

- $G(3, 2) = \frac{A(3, 2)}{G(2, 2)} = \frac{3}{3} = 1$

$$
G = \begin{bmatrix}
2 & 0 & 0 \\
1 & 3 & 0 \\
2 & 1 & 0
\end{bmatrix}
$$

- $A(3, 3) = A(3, 3) - G(3, 2)^2 \cdot G(2, 2)$:
  $A(3, 3) = 5 - (1^2 \cdot 3) = 2$
  $A(3, 3) = 2$
$$ A = \begin{bmatrix} 4 & 2 & 4 \\ 2 & 9 & 3 \\ 4 & 3 & 2 \end{bmatrix} $$ For **Column 3**: 
- $G(3, 3) = \sqrt{A(3, 3)} = \sqrt{2} = \sqrt{2}$ $$ G = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 3 & 0 \\ 2 & 1 & \sqrt{2} \end{bmatrix} $$
- Final Updated $A$: $$ A = \begin{bmatrix} 4 & 2 & 4 \\ 2 & 9 & 3 \\ 4 & 3 & 2 \end{bmatrix} $$
#### Block Cholesky

Just to relate this back to the idea of 



	
	









