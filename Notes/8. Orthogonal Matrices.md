A Matrix is deemed to be orthogonal, if the following is true:

$$
Q^TQ = QQ^T=I
$$
Orthogonal matrices play a role and have good properties which make them useful for solving nearly full rank systems. Some of the geometrical properties are:

$Rotations$ where Q is an orthoganal matrix

$$
\begin{bmatrix}
cos(\theta) & sin(\theta) \\
-sin(\theta) & cos(\theta)
\end{bmatrix}
$$

For $y=Q^Tx$, $y$ is obtained by rotating x counterclockwise through an angle $\theta$.

$Reflection$ where Q is an orthoganal matrix is


$$
\begin{bmatrix}
cos(\theta) & sin(\theta) \\
sin(\theta) & -cos(\theta)
\end{bmatrix}
$$
Reflections and Rotatiosn are computationally attractive because they are easily constructed and because they can be used to introduce zeros in a vector by properly choosing the rotating angle or reflection plane.

There are a couple of very important rotations/reflections
- Householder Reflection-used to introduce 0s into a vector, annihilation of all but the first component of a victor
- Givens rotation- used to introduce 0s to a vector more selectivly

## QR Factorization

A rectangular matrix (does not need to be square) $\in$ $R^{mxn}$ can be factored into a product of an orthogonal matrix $Q \in R^{mxm}$ and upper triangular matrix $R \in R^{mxn}$
$$
A=QR
$$
 - If A has full column rank then Q is unique where it has orthonormla columns and R is upper triangular matrix with positive diagonal entries. Then $R=G^T$, where G is the lower triangular Cholesky factor of $A^TA$
### Types of QR Algorithms

All of these algorithms wont be studied in depth but for completeness they are included. Some algorithms formulations that exist for the QR algorithm are
- Householder QR Algorithm (Block too)
	- Find Householder matrices $H_1,H_2,...H_n$ such that if $Q=H_1,H_2,...H_n$ then $Q^TA=R$ is an upper triangular matrix 
- Block Recursive
- Givens Rotations
	- If $G_j$ denotes the $jth$ Givens rotations in the reductions, then $Q^TA=R$ is upper triangular, where $Q=G_1,G_2,...,G_t$ and t is the total number of rotations
- Hessenberg QR via Givens
	- If A is upper Hessenberg then A is overwritten with $Q^TA=R$ where Q is orthogonal and R is upper triangular. $Q=G_1,G_2,...,G_{n-1}$ is a product of Givens rotations where $G_j$ has the form $G_j=G(j,j+1,\theta_j)$
- Modified Gram-Schmidt Algorithm
	- **Steps**: Let $A = [a_1, a_2, \dots, a_n]$ be the matrix with columns to be orthonormalized. The goal is to compute an orthonormal basis $Q = [q_1, q_2, \dots, q_n]$.
		1. **Initialization**:
		   - Start with the first vector:
     $$
     q_1 = \frac{a_1}{\|a_1\|}
     $$
		   - Normalize $a_1$ to get the first orthonormal vector $q_1$.

		2. **Iterative Orthogonalization**:
		   - For each subsequent vector $a_k$ ($k = 2, 3, \dots, n$):
		     1. Remove the components of $a_k$ in the direction of previously computed orthonormal vectors $q_1, q_2, \dots, q_{k-1}$:
        $$
        r_{ij} = q_i^T a_k \quad \text{(Projection coefficient for $q_i$ onto $a_k$)}
        $$
        $$
        \tilde{a}_k = a_k - \sum_{i=1}^{k-1} r_{ik} q_i \quad \text{(Remove projections of $a_k$)}
        $$
		     2. Normalize the resulting vector to get $q_k$:
        $$
        q_k = \frac{\tilde{a}_k}{\|\tilde{a}_k\|}
        $$

		3. **Repeat**:
		   - Continue until all columns of $A$ are processed.


Before we compare we go into a different viewpoint of the QR Factorization here to help understand in more detail what is going on


### QR Decomposition: Projectors

- Vectors $a_1, a_2, \dots, a_n$ in $n$-dimensional space:
  - $a_1, a_2, \dots, a_n$: Columns of matrix $A$.  
  - $A$: Full rank matrix.  

QR decomposition allows us to express $A$ as:
$$
A = QR
$$
where $Q$ is an orthogonal matrix (its columns form an orthonormal basis for the column space of $A$), and $R$ is an upper triangular matrix.

---

#### Finding the Orthogonal Space
- $q_1, q_2, \dots, q_n$: Use **Gram-Schmidt** process to form the columns of $Q$, ensuring orthogonality.

##### To go from $A = QR$:  
- Compute coefficients:
$$
c_{ij} = q_i^* \cdot a_j
$$
This represents the projection of $a_j$ onto $q_i$.

- The columns of $Q$ form an orthonormal basis, meaning:
$$
Q^T Q = I
$$

---

#### Gram-Schmidt Process

1. Start with $a_1$:  
   - Normalize $a_1$:  
     $$
     q_1 = \frac{a_1}{\|a_1\|}
     $$
   - This creates the first orthonormal vector.

2. To orthogonalize $a_2$:  
   - Project $a_2$ onto $q_1$:  
     $$
     \text{proj}_{q_1}(a_2) = (q_1^* \cdot a_2) q_1
     $$
   - Subtract the projection from $a_2$:  
     $$
     v_2 = a_2 - \text{proj}_{q_1}(a_2)
     $$
   - Normalize $v_2$:  
     $$
     q_2 = \frac{v_2}{\|v_2\|}
     $$

3. Repeat for $a_3, \dots, a_n$:  
   - For $a_j$: Subtract projections onto all previous $q_i$.  
   - Normalize:  
     $$
     q_j = \frac{v_j}{\|v_j\|}
     $$

The process ensures that each $q_i$ is orthogonal to all previous $q_k$ for $k < i$, forming an orthonormal set.

---

##### Q Matrix:
$$
Q = \begin{bmatrix}
q_1 & q_2 & q_3 & \dots & q_n
\end{bmatrix}
$$

---

##### General Projection Formula
Any vector $x$ can be projected onto the span of $q_1, q_2, \dots, q_n$ using the projection matrix $P$:
$$
\text{proj}_Q(x) = P x, \quad P = QQ^*
$$

For example, if $Q$ has three columns:
$$
Q = \begin{bmatrix}
q_{11} & q_{12} & q_{13} \\
q_{21} & q_{22} & q_{23} \\
q_{31} & q_{32} & q_{33}
\end{bmatrix}
$$

Then:
$$
Q^* = \begin{bmatrix}
q_{11} & q_{21} & q_{31} \\
q_{12} & q_{22} & q_{32} \\
q_{13} & q_{23} & q_{33}
\end{bmatrix}
$$

The projection matrix $P$ is:
$$
P = QQ^* = \begin{bmatrix}
q_{11} & q_{12} & q_{13} \\
q_{21} & q_{22} & q_{23} \\
q_{31} & q_{32} & q_{33}
\end{bmatrix}
\begin{bmatrix}
q_{11} & q_{21} & q_{31} \\
q_{12} & q_{22} & q_{32} \\
q_{13} & q_{23} & q_{33}
\end{bmatrix}
$$

---

#### Complementary Projector

Define the complementary projector $I - P$, which finds the component of a vector orthogonal to the span of $Q$:
$$
I - P = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} - P
$$

This is useful for breaking a vector $x$ into:
$$
x = Px + (I - P)x
$$
where $Px$ lies in the span of $Q$, and $(I - P)x$ lies in the orthogonal complement.

---

#### Modified Gram-Schmidt Process

The **modified Gram-Schmidt process** improves numerical stability over the classical version by directly orthogonalizing each vector step-by-step. This avoids accumulating rounding errors when working with floating-point numbers.

### Steps:
1. Start with $a_1$:  
   $$
   q_1 = \frac{a_1}{\|a_1\|}
   $$

2. For each subsequent column $a_j$:  
   - Subtract projections onto all previous $q_i$:
     $$
     v_j = a_j - \sum_{i=1}^{j-1} (q_i^* \cdot a_j) q_i
     $$
   - Normalize $v_j$:  
     $$
     q_j = \frac{v_j}{\|v_j\|}
     $$

The resulting $Q$:
$$
Q = \begin{bmatrix}
q_1 & q_2 & q_3 & \dots
\end{bmatrix}
$$

The resulting $R$:
$$
R = \begin{bmatrix}
r_{11} & r_{12} & r_{13} & \dots \\
0 & r_{22} & r_{23} & \dots \\
0 & 0 & r_{33} & \dots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
$$

---

#### QR via Householder Reflections

Householder reflections provide a numerically stable alternative to Gram-Schmidt for computing the QR decomposition. The Householder reflector is defined as:
$$
Q_k = I - 2\frac{vv^T}{v^T v}
$$
where $v$ is a reflection vector that aligns the column of $A$ with the coordinate axis.

### Steps:
1. At each step $k$, construct $Q_k$ to zero out elements below the diagonal in column $k$.
2. Apply $Q_k$ sequentially to transform $A$ into an upper triangular matrix $R$.

---

#### Principal Component Analysis (PCA) Using QR

In PCA, the QR decomposition is often used to simplify computations. The orthogonal matrix $Q$ provides a basis for projecting data, while the triangular matrix $R$ can be used to determine the directions of maximum variance.

To solve $Ax = b$ in the context of PCA:
1. Decompose $A$ into $Q$ and $R$.
2. Compute:
   $$
   Q^T b = c
   $$
3. Solve $Rx = c$ using back substitution.

---

#### Reflection in Hyperplanes

A Householder reflector is a transformation matrix used to reflect a vector $x$ about a hyperplane orthogonal to a vector $v$. The reflector matrix is:
$$
F = I - 2\frac{vv^T}{v^T v}
$$

For example, if $v = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$:
$$
F = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} - 2 \frac{\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix} \begin{bmatrix}
1 & 2 & 3
\end{bmatrix}}{14}
$$

---

#### Key Stability Notes

- Modified Gram-Schmidt is more stable than classical Gram-Schmidt because it avoids repeated re-projections.
- Householder reflections are even more stable, making them the preferred method for large matrices or when precision is critical.


### Comparison

If speed is of necessity then the MGS is about twice as efficient as the householder orthoganlization. However, if orthonormality is critical, MGS should be used to compute orthonormal bases only when the vector to be orthoganlized is fairly independent. Otherwise the Householder approach should be used.

## Solving Ax=b for Overdetermined systems

In most cases Ax=b has not exact solution since there will typically be more rows than columns so the goal of OLS when $m \geq n$
- $A \in R^{mxn}$
- $b \in R^m$
- $x \in R^n$
is 
$$
minimize||Ax-b||_p
$$
for a certain p. where the norm p sets this objective

$$
||Ax-b||_p = \sum_{i=1}^{n}||(Ax-b)_i||^{1/p}
$$

Intuitively when 
- $p=1$ we are minimuzing the sum of the absolute residulats
- $p= \infty$ we are minimizing the largest residual
- $p=2$ measures the sum of squares residuals

### Good properties of Choosing p=2
- When p is 1 or $\infty$ there isn't a good closed form solution due to the nonlinearity of solving this
- The 2 norm remains preserved under othogonal transformation so we can seek an orthogonal Q such that $||(Q^TA)x-(Q^Tb) ||_2$ is easier to solve
- The minimization in the two norm is a differentable function which satisfies the gradient function $\nabla \phi(x) = 0$ 



---

#### **Objective Function**
The objective function is:
$$
\phi(x) = \frac{1}{2} \|Ax - b\|_2^2
$$
The $\frac{1}{2}$ simplifies gradient computation.

---

#### **Gradient of $\phi(x)$**
The gradient of $\phi(x)$ is:
$$
\nabla \phi(x) = A^T(Ax - b)
$$
Minimizing $\phi(x)$ involves setting $\nabla \phi(x) = 0$, giving:
$$
A^T(Ax - b) = 0
$$
This is the **normal equation**.

---

#### **Normal Equations**
The normal equation is:
$$
A^T A x = A^T b
$$
If $A$ has **full column rank**, $A^T A$ is invertible, and the solution is unique:
$$
x_{\text{LS}} = (A^T A)^{-1} A^T b
$$

---

#### **Implications of Full Rank**
- **Full Column Rank**:
  - $A^T A$ is **symmetric positive definite**.
  - Ensures a unique solution $x_{\text{LS}}$.
- **Null Space**:
  - If $z \in \text{null}(A)$ (i.e., $Az = 0$), adding $z$ does not affect the solution.

---

#### **Minimum Residual and Solution**
The **residual** is:
$$
r_{\text{LS}} = b - Ax_{\text{LS}}
$$
The **minimum residual norm** is:
$$
\rho_{\text{LS}} = \|Ax_{\text{LS}} - b\|_2
$$
This measures how well $Ax_{\text{LS}}$ approximates $b$.

#### Things to consider for OLS in fully rank case

- even if the matrix if full rank, if it is nearly rank deficient than it can lead to issues in the computation
	- The condition number $k_2(A) =\frac{\sigma_{max}}{\sigma_{min}}$ where $\sigma_{max}$ and $\sigma_{min}$ are the smallest and largest singular values of A
		- $\sigma_{max}$ - indicates how much A stretches the vector with the largest impact-scaling factors
		- $\sigma_{min}$ - indicates how much A stretches the vector with the smallest impact
		- $\sigma = \sqrt{\lambda}$
	- **The sensitivity of $x_LS$ is dependent upon $k_2(A)^2$ ** 
		- Small changes in A or b can lead to deviations in $x_{LS}$ that are amplified up to $10^8$ times
- There are multiple methods to solve OLS
	- Method of Normal Equations
		- Steps
			- Compute lower triangular portion of $C=A^TA$
			- Form the matrix product $d=A^Tb$
			- Compute the Cholesky factorization $C=GG^T$
			- Solve $Gy=d$ and $G^Tx_{LS}=y$
		- The method of normal equation can break down on matrices that are not particularly close to being numerically rank deficient
		- Produces an $x_{LS}$ whose error depends on the condition error, a facotr than can be larger than the condition number with a small residual LS Problem
	- QR Factorization
		- The Householder method will break down in back substitution if rank(A) <n. 
		- The approach solves a nearby LS problem so the methods produced here are a computed solution with relative error that is predicted by the condition of the underlying LS problem.
		- Use if b is close to the span of the columns of A
	- Comparing
		- Normal approach involves about half the arithmetic when m>>n and doesnt require as much storage
		- QR approach is more applicable to a wider class of LS problems due to the issues causes by the condition number being $\frac{1}{\sqrt{u}}$ but QR is in trouble if condition number is $\frac{1}{{u}}$

### Summary of Flop Counts for OLS for Overdetermined Systems

For comparison of different algorithms here are the flop counts of them from **Matrix Computations**

#### Flops Associated with Various Least Squares Methods- Rank Deficient

| **LS Algorithm**               | **Flop Count**                      |
|---------------------------------|-------------------------------------|
| Normal equations                | $mn^2 + \frac{n^3}{3}$              |
| Householder QR                  | $\frac{n^3}{3}$                     |
| Modified Gram-Schmidt           | $2mn^2$                             |
| Givens QR                       | $3mn^2 - n^3$                       |
| Householder Bidiagonalization   | $4mn^2 - 2n^3$                      |
| $R$-Bidiagonalization           | $2mn^2 + 2n^3$                      |
| SVD                             | $4mn^2 + 8n^3$                      |
| $R$-SVD                         | $2mn^2 + 11n^3$                     |

---
#### Flops Associated with Various Methods for Square Linear Systems

| **Method**                   | **Flop Count**   |
| ---------------------------- | ---------------- |
| Gaussian elimination         | $\frac{2n^3}{3}$ |
| Householder QR               | $\frac{4n^3}{3}$ |
| Modified Gram-Schmidt        | $2n^3$           |
| Singular value decomposition | $12n^3$          |

---
#### Correlating System to Rank

| **System Type**     | **Likelihood of Full Rank** | **Reason**                                                                                           |
| ------------------- | --------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Overdetermined**  | High                        | Typically, more rows (data points) than columns (variables) ensure that columns are independent.     |
| **Square**          | Depends on \( A \)          | Full rank occurs if the matrix is non-singular ${det}(A) \neq 0$                                     |
| **Underdetermined** | Moderate to Low             | Fewer rows than columns make row independence less likely, increasing the chance of rank deficiency. |

This is just the start of the study of OLS since there are many different types of alternative methodologies that can be used to compute this such as Ridge Regression, Constraints,Weighted. In the future, time permitting these will be studeied in more detail such as in Chapter 6 of **Matrix Computation**


