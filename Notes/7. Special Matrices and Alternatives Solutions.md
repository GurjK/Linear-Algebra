
## Special Matrices

A banded matrix is a matrix where all the elements that are nonzero appear around the diagonal. Then the matrix has an upper bandwidth, # of elements above the diagonal that are non-zero, q. Lower bandwidth, # of elements below the diagonal that are non-zero, p.

If the matrix A is banded and it has an LU decomposition then the matrix U has an upper bandwidth q and lower bandwidth p.

Some applications where a banded matrix can arise is in the finite difference approximation method for pricing. One method that we will look at in particular is solving Ax=b where A is symmetric positive definite. Usually the cholesly facotorization is used for this however, due to the banded nature of this, we can adjust this algorithm to make it more elegant. 

From here we can come up with even more efficient algorithms that exist for 
- Banded
- Triadiagonal
- Indefinite
- Block Matrices



**Please Check Matrix Computation for the implmentations and Algorithms for these**

## Toeplitz Systems

A matrices whose entries are constant along the diagonal are called Toeplitz System. The values not on the diagonal can be anything. There are three efficient algorithms to solve this assuming the matrix is positive definite:
 - Durbins's Algorithm for $T_ny = -[r_1,.....,r_n]^T$
 - Levinson's Algorithm for $T_nx=b$ -symmetriic
 - Trench's Algorithm for $B=T_n^-1$ -if no symmetry

Again the exact implementations are shown in Matrix Computation if needed. These are more efficient than cholesky if the matrix is Toeplitz

## Eigenvalue Decompositions

Say we have a matrix A and it is nonsingular and we wish to solve $Au=b$
then if we let 
- V = eigenvectors of A
- $\Lambda$ = diagonal matrix with eigenvalues, $\lambda_i$
then
$$
V^{-1}AV = \Lambda 
$$
and u can be solved as follows
$$
u = A^{-1}b = (V^{-1}\Lambda V)^{-1}b = V(\Lambda^{-1}(V^{-1}b))
$$

This means that A has a *fast eigenvalue decomposition* that is as efficient to rival Cholesky or Gaussian elimination if
 - Matrix-vector products of the form $y=Vx$ require *O(nlogn)* flops
 - The eigenvalues $\lambda_1,....,\lambda_n$ require *O(nlogn)* flops to evaluate
 - Matrix vector products of the form $\hat{b} = V^{-1}b$

The matrix V is associated with the discrete Fourier Transforms, sin, and cosine transforms. In order to understand this we must firm look into Fourier Transforms. 

## Fourier Transforms

A Fourier Transform is a mathematical operation that allows a function to be approximated as a sum of sine and cosine functions at specific frequencies. These sine and cosine functions form the basis of the frequency domain, enabling analysis of a signal's frequency content. This is particularly useful in understanding the behavior of time-domain signals, such as sound or electrical signals, as they vary over time.

The Fourier Transform bridges two domains: the time domain, which describes how a signal changes over time, and the frequency domain, which describes the frequencies that compose the signal. The concept of periodicity is central to this, where functions repeat at regular intervals. For instance, $\sin(2\pi t)$ has a period of $T = 1$ second. The transform uses the concept of frequency, measured in Hertz ($Hz$), to quantify how often a wave oscillates in one second. For example, a wave completing one oscillation per second corresponds to $1 \, Hz$.

The Fourier Series represents periodic functions by projecting them onto an orthogonal basis of sine and cosine functions. A function $f(x)$ can be written as a sum of sine and cosine terms with coefficients $a_k$ and $b_k$, representing the amplitudes of the cosine and sine components, respectively. The general formula for the Fourier Series is:

$$
f(x) = a_0 + \sum_{k=1}^\infty \left[ a_k \cos(kx) + b_k \sin(kx) \right].
$$

These coefficients are determined by integrals over the functionâ€™s domain. Specifically, 

$$
a_k = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(kx) dx, \quad b_k = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(kx) dx.
$$

The term $a_0$ represents the DC component, or the average value of the function.

For non-periodic functions, the Fourier Transform generalizes this approach, converting a time-domain signal into its frequency spectrum. The transform is defined as:

$$
F(w) = \int_{-\infty}^\infty f(x) e^{-iwx} dx, \quad f(x) = \int_{-\infty}^\infty F(w) e^{iwx} dw.
$$

This transformation provides information about the amplitude and phase of each frequency component. Key properties of the Fourier Transform include its ability to simplify differential equations, where derivatives in the time domain correspond to multiplications in the frequency domain: 

$$
F\left( \frac{df}{dx} \right) = iwF(w).
$$ 

Fourier Transforms are widely used in applications such as noise filtering, signal processing, and solving partial differential equations.

---

## Discrete Fourier Transform (DFT)

The Discrete Fourier Transform (DFT) approximates the Fourier Transform for discrete data points, making it suitable for computational applications. It converts a sequence of $N$ time-domain data points into their frequency-domain representation. The DFT is defined mathematically as:

$$
f_k = \sum_{j=0}^{N-1} f_j e^{-i2\pi jk/N}, \quad k = 0, 1, ..., N-1.
$$

Here, $f_k$ are the Fourier coefficients, representing the frequency components, and $N$ is the number of data points. The DFT can be expressed in matrix form using the Fourier Vandermonde matrix, $\mathbf{W}$, as follows:

$$
\mathbf{F} = \mathbf{W} \cdot \mathbf{f}.
$$

The matrix $\mathbf{W}$ is defined by the roots of unity, $\omega_N = e^{-2\pi i/N}$, and has the following structure:

$$
\mathbf{W} = 
\begin{bmatrix}
1 & 1 & 1 & \cdots & 1 \\
1 & \omega_N & \omega_N^2 & \cdots & \omega_N^{N-1} \\
1 & \omega_N^2 & \omega_N^4 & \cdots & \omega_N^{2(N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \omega_N^{N-1} & \omega_N^{2(N-1)} & \cdots & \omega_N^{(N-1)(N-1)}
\end{bmatrix}.
$$

Each entry of the matrix corresponds to the multiplication of data points with their respective weights in the frequency domain. The computation of the DFT requires $O(N^2)$ operations, making it computationally expensive for large datasets.

---

## Fast Fourier Transform (FFT)

The Fast Fourier Transform (FFT) optimizes the DFT by reducing the complexity from $O(N^2)$ to $O(N \log N)$. It achieves this through a divide-and-conquer approach, breaking down the computation into smaller recursive problems.

The FFT can be expressed as a matrix product, emphasizing its recursive nature. The $N \times N$ Fourier matrix is decomposed as:

$$
\mathbf{F}_N =
\begin{bmatrix}
\mathbf{F}_{N/2} & \mathbf{D}_{N/2} \cdot \mathbf{F}_{N/2} \\
\mathbf{F}_{N/2} & -\mathbf{D}_{N/2} \cdot \mathbf{F}_{N/2}
\end{bmatrix}.
$$

Here:
- $\mathbf{F}_{N/2}$ represents the Fourier transform of the even and odd components.
- $\mathbf{D}_{N/2}$ is a diagonal matrix containing the twiddle factors ($\omega_N^k = e^{-2\pi i k / N}$):

$$
\mathbf{D}_{N/2} =
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & \omega_N^1 & 0 & \cdots & 0 \\
0 & 0 & \omega_N^2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \omega_N^{N/2 - 1}
\end{bmatrix}.
$$

By splitting into even and odd components, the FFT can also be written as:

$$
\hat{\mathbf{f}} =
\begin{bmatrix}
\mathbf{I} & \mathbf{D}_{N/2} \\
\mathbf{I} & -\mathbf{D}_{N/2}
\end{bmatrix}
\cdot
\begin{bmatrix}
\mathbf{F}_{\text{even}} \\
\mathbf{F}_{\text{odd}}
\end{bmatrix}.
$$

This block matrix highlights how the FFT reduces redundant computations by reusing smaller transforms recursively. The recursion stops when $N = 2$, at which point the DFT is directly computed.

*These notes were created based off the work in Data Driven Science & Engineering by Brunton


## Fast Matrix-Vector Products

Now that we understand what fourier transforms are we will now look at their application in linear algebra. We will realign will our normal notation for this section

The **Discrete Fourier Transform (DFT)** of a vector x is a matrix-vector product

$$
y=F_nx
$$
where DFT matrix $F_n = f_{kj}$ where 
- $f_{kj} = w_n^{(k-1)(j-1)}$ 
- $w_n = e^{(-2\pi i /n)}=cos(2\pi /n)-isin(2\pi / n)$

If n is highly composite, then it is possible to carry out the DFT in many fewer than $O(n^2)$ flops required by conventional matrix-vector multiplication

## Circulant and Discrete Poisson Systems

Now return back to the topic of the eigenvalue decomposition and the V matrix which is the eigenvectors of the matrix A. If we remember what the criteria number 3 for using fast eigenvalue decomposition was, *Matrix-vecotr product of the form $\hat{b}=V^{-1}b$ require O(nlogn) flops to evaluate*, we notice that we actually have a method for this, the inverse fourier transform. 

After recognizing that the DFT is a scaled unitary matrix we come to the conclusion 

$$
y = F_n^{-1}x=\frac{1}{n}\hat{F_n}x 
$$
- Scaled Unitary Matrix Satisfies $A^HA=cI$
- $nI_n = F_n^HF_n=\hat{F_n}F_n$

A Circulent matrix is a Toeplitz matrix with wraparound so 

$$ C = \begin{bmatrix} c_0 & c_1 & c_2 & \cdots & c_{N-1} \\ c_{N-1} & c_0 & c_1 & \cdots & c_{N-2} \\ c_{N-2} & c_{N-1} & c_0 & \cdots & c_{N-3} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ c_1 & c_2 & c_3 & \cdots & c_0 \end{bmatrix}. $$

and for context a downward shifted permutation matrix is a matrix where its a matrix where all the elements are shifted one entry down. Any Circulant matrix is a combination of the identity matrix and the downward shifted permutation matrix. 

- If $V=F_n$ then $V^{-1}D_nV =\Lambda = diag(\lambda_1,...\lambda_n)$
- where $\lambda_{j+1}=\hat{w}_n^j=cos(\frac{2j\pi}{n}+isin(\frac{2j\pi}{n})$

Eventually we come to the point where the eigenvalues of the Circulent matrix C(z) are the components of the vector $F_nz$

Algorithm for C(z)x=y 
- Use an FFT to compute $c=F_ny$ and $d=F_nz$
- $w=\frac{c.}{d}$
- Use an FFT to compute $u=F_nw$
- $x=\frac{u}{n}$

## FFT for PDEs

This wont be in depth at the moment but as mentioned above FFT can be used to solve a PDE. For example 

$$
\frac{d^{2}}{dx^{2}} =-f(x)
$$
where $\alpha \leq u(x) \leq \beta$
in addition with 4 possible boundary which correspond to either the boundaries being equal, unique, first derivatives of both or only one derivative.

This can be transformed into a linear equation using the finite difference method, however it isn't as useful compared to Gaussian which is O(n) vs O(nlogn). The more applicable cases are where there is a partial derivative involes as follows

... to be expanded upon potentially at a later date. 


