# Optimizing Matrix-Matrix Multiplication: Understanding Algorithm Design and Performance

## Processor Architecture and Key Units

1. **CPU (Central Processing Unit)**:
   - The computational unit responsible for general-purpose processing.
   - Initiates most computations and delegates specialized operations.

2. **FPU (Floating Point Unit)**:
   - Dedicated hardware for performing arithmetic operations involving floating-point numbers.
   - Operations like addition, multiplication, and division of floating-point numbers occur here.
   - Important for matrix computations due to the heavy reliance on decimal arithmetic.

3. **Memory Hierarchy**:
   - **Registers**: The smallest, fastest memory available directly within the processor.
   - **Cache Levels (L1, L2, L3)**:
     - **L1 Cache**: Closest to the CPU, extremely fast but limited capacity.
     - **L2 Cache**: Slightly larger, slower than L1.
     - **L3 Cache**: Shared across cores, larger but slower than L2.
   - **Main Memory (RAM)**: Much larger but significantly slower than caches. Accessing main memory is costly in terms of latency.
   - **Key Point**: Efficient algorithms optimize for cache usage to reduce expensive memory accesses.

4. **Data Movement Costs**:
   - Bringing data from main memory to registers is expensive.
   - Algorithms must minimize these transfers to maximize performance.

---

## Matrix-Matrix Multiplication (MMM)

- **Concept**: The core of many numerical computations, MMM involves multiplying matrices $A$ and $B$ to produce $C$.
- **Challenges**:
  - Computationally intensive with significant memory access requirements.
  - Limited by memory bandwidth rather than raw computational speed.
- **Optimization Strategies**:
  - Partition matrices into smaller blocks that fit within L1 or L2 cache.
  - Perform operations on these smaller blocks to reduce data movement.
- **Performance Metric**:
  - Measured in GFlops (Billion Floating Point Operations per Second).

---

## Key Computational Operations

1. **Dot Product ($x^T \cdot y$)**:
   - **Memory Operations**: $2n$ memops.
   - **Floating Point Operations**: $2n$ flops.
   - **Ratio**: $1:1$ (flops to memops).

2. **AXPY (Scalar-Vector Addition)**:
   - **Operation**: $\alpha x + y = z$.
   - **Memory Operations**: $3n$ memops.
   - **Floating Point Operations**: $2n$ flops.
   - **Ratio**: $2:3$.

3. **Matrix-Vector Multiplication ($C = A \cdot B$)**:
   - **Memory Operations**: $n^2$ memops.
   - **Floating Point Operations**: $2n^2$ flops.
   - **Ratio**: $1:2$.

4. **Matrix-Matrix Multiplication ($C = AB$)**:
   - **Memory Operations**: $2n^2$ memops.
   - **Floating Point Operations**: $2n^3$ flops.
   - **Ratio**: $\frac{2}{n}$ (improves as $n$ increases).

5. **Rank-1 Update**:
   - Used to update the matrix by an outer product.
   - **Memory Operations**: $2n^2$ memops.
   - **Floating Point Operations**: $2n^2$ flops.
   - **Ratio**: $1:1$.

---

## Optimization Goals and Principles

1. **Data Locality**:
   - Perform computations on data already loaded in the cache to avoid redundant memory accesses.

2. **Blocking**:
   - Partition matrices into smaller sub-matrices (blocks).
   - Load these blocks into faster cache memory for repeated operations.

3. **Peak Performance**:
   - By leveraging cache-optimized algorithms, achieve up to 90% of theoretical peak performance.
   - Example: Avoid the naive $2/3$% performance of unoptimized matrix operations.

4. **Use Efficient Libraries**:
   - Libraries like **BLAS (Basic Linear Algebra Subprograms)** and **FLAME** implement these optimizations.
   - Check these libraries for further guidance on high-performance numerical computations.

---


# Conditioning and Stability

For a function:
$$
y = f(x), \quad r = x + \Delta x, \quad f(r) \neq f(x)
$$

## **Conditioning**  
Describes the behavior of a problem under perturbation (how sensitive the output is to changes in the input).

#### **Stability**  
Describes the behavior of the algorithm under perturbation.

1. **Ill-conditioned**  
   Small changes in $x$ result in large changes in $f(x)$.  
   
2. **Well-conditioned**  
   Small changes in $x$ result in small changes in $f(x)$.

---

#### **Absolute Condition Number**
For $f(x)$:
- A perturbation is added, so:
$$
\Delta f = f(x + \Delta x) - f(x)
$$

- The **absolute condition number** $K$ is defined as:
$$
K = \lim_{\Delta x \to 0} \frac{\|\Delta f\|}{\|\Delta x\|} = \left\| \frac{\partial f}{\partial x} \right\|
$$

- We want $K$ to be **small** so that the problem is well-conditioned.

---

#### **Jacobian Matrix** (For differentiable $f$)
If $f$ is differentiable:
$$
\mathbf{J} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$

This represents a **coordinate transformation**. The Jacobian maps small vectors $\Delta x$ to $\Delta f$.

For small $\Delta x$:
$$
\Delta f \approx \mathbf{J}(x) \Delta x
$$

---

#### **Absolute Condition Number**
$$
K = \|\mathbf{J}(x)\|, \quad \text{where } \mathbf{J} \text{ is the Jacobian.}
$$

#### **Relative Condition Number**
$$
K = \frac{\|f(x)\|}{\|x\|} \quad \text{(well-conditioned if } K \approx 10^0, \text{ ill-conditioned if } K \sim 10^6 \text{ to } 10^{16}\text{).}
$$

---

##### For a Matrix

#### **Relative Condition Number**
$$
K = \|\mathbf{A}\| \|\mathbf{A}^{-1}\|
$$

If $\mathbf{A}$ is square and nonsingular:
$$
1 \leq \|\mathbf{A}\| \|\mathbf{A}^{-1}\| \leq \infty
$$

The condition number $K$ satisfies:
$$
K \leq \|\mathbf{A}\| \|\mathbf{A}^{-1}\|
$$

#### **2-Norm of a Matrix**
The 2-norm of a matrix is the **largest singular value**:
$$
\|\mathbf{A}\|_2 = \sigma_{\max}
$$

Thus:
$$
K(\mathbf{A}) = \|\mathbf{A}\| \|\mathbf{A}^{-1}\| = \frac{\sigma_{\max}}{\sigma_{\min}}
$$

---

##### Stability of Algorithms

Given $x = \hat{x} + O(10^{-16})$ (numerical roundoff approximation):

1. **Absolute Error**:
   $$ 
   \|\hat{f}(a) - f(x)\| \sim O(10^{-16}) 
   $$

2. **Relative Error**:
   $$ 
   E = \frac{\|\hat{f}(a) - f(x)\|}{\|f(x)\|} 
   $$

---

##### Example: Numerical Schemes

1. **Forward Euler Scheme**:
   $$ 
   y_{n+1} = y_n + \Delta t \cdot f(t_n, y_n)
   $$

2. **Backward Euler Scheme**:
   $$ 
   y_{n+1} = y_n + \Delta t \cdot f(t_{n+1}, y_{n+1})
   $$

- Stability analysis depends on eigenvalues of $f$.  
- Numerical stability is achieved if eigenvalues of the method fall within the **stability region**.

---

##### Topic 3: Least Squares and Overdetermined Systems

For $Ax = b$:
- If $\mathbf{A}$ is overdetermined (more rows than columns), solve:
  $$ 
  \min_x \|\mathbf{A}x - \mathbf{b}\| 
  $$

- This involves projecting $b$ onto the **range of $\mathbf{A}$**:
  $$ 
  b \to \mathbf{A}x = \text{projection of } b \text{ onto range of } \mathbf{A}.
  $$

---

##### SVD and Conditioning

For $\mathbf{A}$:
$$ 
\mathbf{A} = \mathbf{U} \Sigma \mathbf{V}^*
$$

If $\mathbf{A}$ changes slightly, $\Sigma$ (singular values) will feel this change directly, leading to instability if $\Sigma$ has small or zero values.

###### **Condition Number via SVD**
$$
K = \frac{\sigma_{\max}}{\sigma_{\min}}
$$
where $\sigma_{\max}$ and $\sigma_{\min}$ are the largest and smallest singular values of $\mathbf{A}$.

---

### Randomized Linear Algebra

For $\mathbf{A} \in \mathbb{R}^{m \times n}$, where $m \gg n$:
- The goal is to sample the column space efficiently by applying **random projections**.  
- Compute:
  $$ 
  \mathbf{Y} = \mathbf{A} \mathbf{\Omega}, \quad \mathbf{\Omega} \in \mathbb{R}^{n \times r}
  $$

  where $\mathbf{\Omega}$ is a random matrix.

- This reduces $\mathbf{A}$ to a lower-dimensional subspace:
  $$ 
  \mathbf{A} \approx \mathbf{Q} \mathbf{R}, \quad \mathbf{Q} \in \mathbb{R}^{m \times r}
  $$

---

### Random Projection and Low-Rank Approximation

1. **Stage A**:
   - Sample random combinations of columns from $\mathbf{A}$.  
   - Compute a smaller, low-rank representation $\mathbf{Q}$.

2. **Stage B**:
   - Form $\mathbf{B} = \mathbf{Q}^T \mathbf{A}$, where $\mathbf{B}$ captures the key information in $\mathbf{A}$.

---

##### Key Points on Conditioning

1. Orthogonal bases like $Q$ (from QR decomposition) improve numerical stability.  
2. Condition number:
   - $K = 1$ (well-conditioned-perfectly).  
   - $K \gg 1$ (ill-conditioned).

3. Full-rank least squares has the following conditioning:
   $$ 
   K(A) = \|\mathbf{A}\| \|\mathbf{A}^{-1}\|
   $$


## Additional Notes

1. **Trade-Offs**:
   - Balancing computation and memory usage is key to achieving efficiency.
   - Larger matrix sizes often require more sophisticated caching strategies.

2. **Measuring Impact**:
   - The effectiveness of optimizations is quantified in terms of GFlops/second.
   - Real-world performance is limited by factors like hardware architecture and memory bandwidth.

3. **Future Exploration**:
   - Research new algorithms that improve on blocking and cache usage.
   - Explore vectorized instructions and hardware accelerators (e.g., GPUs) for further speedups.
