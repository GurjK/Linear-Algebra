Direct methods have been covered so far to this points in all of the notes, however there are another type of solvers that are less computationally expensive in certain cases without the necessary guarantee that you arrive at the expected answer, these methods are called iterative methods. 

Iterative methods, typically are a routine or algorithm which start with an initial guess and continue until they converge to a certain point. A major advantage of these methods is that especially for sparse matrices they can take advantage of the structure and solve bigger systems more efficiently. 

The list of methods is extensive but I will try to cover some of them below that are relevant and popular but first a summary of some thing that have been discussed so far and some necessary background information is presented:

## Matrix Decompositions

1. **Gaussian Elimination**  
   Complexity: $O(n^3)$

2. **LU Decomposition**  
   $A = LU$  
   Complexity: $O(n^2)$

3. **QR Factorization**  
   $A = QR$  
   - $Q$: set of orthogonal (columns orthogonal and unit length)  
   - $R$: upper triangular matrix  

   $QRx = y \rightarrow Rx = Q^\top y \rightarrow Q^\top Q = I$

4. **Eigenvalue Decomposition**  
   $A = V\Lambda V^{-1}$  
   $AV = V\Lambda \Rightarrow A = V\Lambda V^{-1}$

5. **Singular Value Decomposition**  
   $A = U\Sigma V^\top$  
   - $U$: unitary (2 separate coordinate systems)  
   - $V$: unitary  

6. **Cholesky - Symmetric Matrices**

---

#### Linear Operator Properties

$Ax = b$  
- $A$: Linear operator  

Properties:  
- $A + B = B + A$ (commutative)  
- $A + (B + C) = (A + B) + C$ (associative)  
- $A(BC) = (AB)C$ (distributive)  
- $A0 = B0 = 0$

---

#### Solvability and Regularization

$Ax = b$  
$A \in \mathbb{R}^{m \times n}$:  
- **Start here, then up**

1. **Underdetermined ($m < n$):**  
   - More columns than rows  
   - $\min \|x\|_2$, subject to $Ax = b$

2. **Overdetermined ($m > n$):**  
   - More rows than columns  
   - $\min \|Ax - b\|_2$  
   - $\min \|Ax - b\|_2 + \lambda\|x\|_2^2$ (hyperparameter $\lambda$)

---

#### Notes

Norms:  
$\|x\|_2 = \sqrt{x_1^2 + x_2^2}$ (2-norm = flying)  
$\|x\|_1 = |x_1| + |x_2|$ (1-norm = city-block)  
$\|x\|_\infty = \max \{|x_1|, |x_2|\}$

##### Visualizations:  
- $L_2$ norm: circle  
- $L_1$ norm: diamond  
- $L_\infty$ norm: square where $|x| \leq \infty$


#### Fredholm Alternative

- $Ax = b \Rightarrow A^\ast$: Adjoint of $A$  
- $Ax = b$ if $A^\ast y = 0$: Null space ($0$ eigenbasis of $A$)  

$A^\ast y = 0$:  
1. $\gamma$ on eigenbasis corresponding to invariant quantity  
2. $\Delta x = b, Ax = 0$: Eigenvalue/null space ($\gamma y + b$)


#### Role of Indeterminacy:  
- Number of solutions  
- Solution ambiguity  


---


### Background of Eigenvalues and Eigenvectors for iterative methods

In iterative methods, you apply a matrix A, to a vector repeatedly. When this happens we have one of 2 case scenarios which can occur:
- $|\lambda| < 1$ 
	- $A^iv=\lambda^iv$ vanishes which leads to this converging to 0
- $|\lambda| >1$
	- $A^iv \to \infty$ 

So if we want x to converge to 0 then $k(A)=max(\lambda)$ so small eigenvalues are ideal for this. Another way to think about this idea is that a vector can be described as a linear combination of eigenvectors where you need independent eigenvectors. In symmetric postive definite matrices, the eigenvetors are always positive.

**A key thing to note is that for an iterative system to converge with certaintity we need row diagonal dominance, like in the Jacobi and Gauss-Siedel methods**

This is related to the idea of eigenvalues since the convergence of an iterative method is influenced by the spectral radius or the largest absolute eignevalues. Depending on the method we have to look at the absolute value of the largest eigenvalue and if it in some range then we can ensure that it converges, so if the matrix is row diagonally dominant then we know that it will result in a small spectral radius for the iteration matrix T. 
### Background information of Krylov Subspaces

The idea of Krylov Subspaces originates from the fact or goal that we wish to use a sequence of subspaces to approximate the solution of a linear system $Ax=b$

$$
K_k(A,b) = span(b,Ab,Ab^2,...,A^{k-1}b)
$$
All of these vectors are orthogonal to each other which is the power of this method. In matrix form it can be written as 

$$
K_n = \begin{bmatrix}
| & | & | & ... \\
b & Ab & Ab^2 & ... \\
| & | & | & ...
\end{bmatrix} = Q_nR_n
$$




The entire subspace can be huge, so a subspace of $R^n$ can be reduced to a subspace of size k instead of n. This method is used for iterative solvers and for eigenvalue problems. For large and sparse systems this method is cheap and readily available. If we have repeated eigenvalues then it means that the subspace is smaller.


## Background for Further Sections- Fourier Analysis

# Fourier Transforms

## Definition
- A function can be approximated by sums of sine and cosine terms of certain frequencies.
- Sine and cosine form the basis in the function space, analogous to coordinate systems.

## Key Concepts
- **Frequency**: How often oscillations occur in time.  
  Example: A 1 Hz frequency corresponds to one full oscillation per second.  
- **Periodicity**: Functions that repeat themselves at regular intervals.  
- **Orthogonality**: Sine and cosine functions of different frequencies are orthogonal:
  $$
  \int_{-\pi}^\pi \sin(kx) \cos(kx) dx = 0
  $$

## Time vs. Frequency Domain
- **Time domain**: Describes how a function behaves over time (e.g., $f(x)$).
- **Frequency domain**: Describes how much of each frequency is present in the function.

## Euler's Formula
$$
e^{i\omega x} = \cos(\omega x) + i\sin(\omega x)
$$

---

# Fourier Series

## Definition
Represents a $2\pi$-periodic function $f(x)$ as a sum of sine and cosine terms:
$$
f(x) = a_0 + \sum_{k=1}^\infty \left[ a_k \cos(kx) + b_k \sin(kx) \right]
$$

## Coefficients
- Coefficients $a_k$ and $b_k$ are projections of $f(x)$ onto the sine and cosine basis:
  $$
  a_0 = \frac{1}{2\pi} \int_{-\pi}^\pi f(x) dx
  $$
  $$
  a_k = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \cos(kx) dx
  $$
  $$
  b_k = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(kx) dx
  $$

## Special Cases
- If $f(x)$ is even: $b_k = 0$.  
- If $f(x)$ is odd: $a_k = 0$.

## Application
Fourier series can represent any periodic, piecewise-smooth function.

---

# Fourier Transform

## Definition
Converts a function $f(x)$ from the time domain to the frequency domain.

## Equations
- **Forward Transform**:
  $$
  \hat{f}(\omega) = \int_{-\infty}^\infty f(x) e^{-i\omega x} dx
  $$
- **Inverse Transform**:
  $$
  f(x) = \frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(\omega) e^{i\omega x} d\omega
  $$

## Frequency Representation
Frequencies are given by $\omega_k = \frac{2\pi k}{L}$, where $L$ is the period.

---

# Discrete Fourier Transform (DFT)

## Definition
Approximates the Fourier Transform for discrete data points instead of continuous functions.

## Equation
$$
F_k = \sum_{n=0}^{N-1} f_n e^{-i \frac{2\pi k n}{N}}
$$

## Matrix Representation
The DFT can be written as:

$$
\mathbf{F} =
\begin{bmatrix}
F_0 \\
F_1 \\
F_2 \\
\vdots \\
F_{N-1}
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 1 & \dots & 1 \\
1 & \omega & \omega^2 & \dots & \omega^{N-1} \\
1 & \omega^2 & \omega^4 & \dots & \omega^{2(N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & \omega^{N-1} & \omega^{2(N-1)} & \dots & \omega^{(N-1)(N-1)}
\end{bmatrix}
\begin{bmatrix}
f_0 \\
f_1 \\
f_2 \\
\vdots \\
f_{N-1}
\end{bmatrix}
$$

- Here, $\omega = e^{-i \frac{2\pi}{N}}$.

## Inverse DFT
$$
f_n = \frac{1}{N} \sum_{k=0}^{N-1} F_k e^{i \frac{2\pi k n}{N}}
$$

---

# Fast Fourier Transform (FFT)

## Definition
An efficient algorithm for computing the DFT, reducing the complexity from $O(N^2)$ to $O(N \log N)$.

## Divide and Conquer
- Splits the input into even and odd-indexed elements:
  $$
  F_k = F_{\text{even},k} + e^{-i \frac{2\pi k}{N}} F_{\text{odd},k}
  $$
- For $k = 0, 1, \dots, N/2 - 1$, the result is:
  $$
  F_{k+N/2} = F_{\text{even},k} - e^{-i \frac{2\pi k}{N}} F_{\text{odd},k}
  $$

## Matrix Decomposition
FFT optimizes DFT matrices by splitting into smaller submatrices for even and odd elements.

## Efficiency
FFT is most efficient when $N$ is a power of 2.

---

# Applications of Fourier Analysis

1. **Signal Processing**:
   - Removing noise by filtering high-frequency components.
2. **Image Processing**:
   - JPEG compression uses FFT.
3. **Audio Analysis**:
   - Separating frequency components in music or speech.
4. **Differential Equations**:
   - Solving equations efficiently in the frequency domain.

---

# Key Points to Remember

1. **Orthogonality**:
   - $\sin(nx)$ and $\cos(nx)$ are orthogonal.
2. **Frequency Representation**:
   - High $k$ corresponds to high frequencies.
3. **Time Complexity**:
   - DFT: $O(N^2)$, FFT: $O(N \log N)$.

