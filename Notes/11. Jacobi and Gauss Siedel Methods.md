
# Jacobi and Gauss-Seidel Methods

## Overview of Jacobi and Gauss-Seidel Methods

Both **Jacobi** and **Gauss-Seidel** are **iterative methods** for solving systems of linear equations $Ax = b$. They are particularly useful for large, sparse systems where direct methods (e.g., Gaussian elimination) are computationally expensive.

### Jacobi Method

1. **Idea**:
   - Updates all variables independently and simultaneously in each iteration using values from the **previous iteration**.
   - Based on splitting the matrix $A$ into $D + (L + U)$, where:
     - $D$: Diagonal part of $A$,
     - $L$: Lower triangular part of $A$,
     - $U$: Upper triangular part of $A$.

   The update formula for $x_i^{(k+1)}$ is:
   $$
   x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
   $$
- Another way t look at this is as follows: We can split the matrix into 2 parts instead of 3 where D is a matrix with only the diagonal elements and E is a matrix with the non diagonal elements so $A=D+E$

$$
Ax=b \to Dx=-Ex+b \to x=-D^{-1}Ex+D^{-1}b
$$
$$
x=Bx+z 
$$
where

$$
B=-D^{-1}E,z=D^{-1}b
$$

The starting vector $x_0$ generates a sequence of vectors, where $x_{i+1}$ is closer to the solution. Each error term is $e_i$ so $x_i=x+e_i$ so $x_i+1=Bx_i+z$ which eventually gives you that $e_{i+1}=Be_i$. Each iterate effects the error term but does not affect the correct part of $x_i$ The initial vector $x_0$ has no affect on the solution at all. 

1. **Features**:
   - Fully parallelizable (each variable can be updated independently).
   - Converges slower compared to Gauss-Seidel in many cases.

2. **Convergence Requirements**:
   - The matrix $A$ must be **diagonally dominant** or **symmetric positive definite**.

---

### Gauss-Seidel Method

1. **Idea**:
   - Updates variables sequentially, using the most recent values as soon as they are computed.
   - Based on splitting $A = (D + L) + U$, the formula for $x_i^{(k+1)}$ is:
   $$
   x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
   $$

2. **Features**:
   - Typically converges faster than Jacobi because it uses updated values immediately.
   - Less parallelizable since updates depend on previously computed values.

3. **Convergence Requirements**:
   - The matrix $A$ must be **diagonally dominant** or **symmetric positive definite**.

---

### Key Differences

| **Feature**        | **Jacobi Method**                         | **Gauss-Seidel Method**                  |
|--------------------|------------------------------------------|-----------------------------------------|
| **Updates**        | All variables updated simultaneously.    | Variables updated sequentially.         |
| **Convergence Speed** | Slower.                               | Typically faster.                       |
| **Parallelization**| Fully parallelizable.                    | Not easily parallelizable.              |
| **Dependency**     | Uses only values from the previous iteration. | Uses updated values during iteration.   |

---

## Example: Solve a $3 \times 3$ System

We solve the system of linear equations:
$$
4x_1 - x_2 + x_3 = 3
$$
$$
-x_1 + 3x_2 - x_3 = 9
$$
$$
x_1 - x_2 + 5x_3 = -6
$$

### Matrix Form

$$
A = \begin{bmatrix} 
4 & -1 & 1 \\
-1 & 3 & -1 \\
1 & -1 & 5 
\end{bmatrix}, \quad
b = \begin{bmatrix} 
3 \\
9 \\
-6 
\end{bmatrix}
$$

---

### Jacobi Method

#### Iteration Formula
For the Jacobi method, the formulas for each variable are:
$$
x_1^{(k+1)} = \frac{1}{4} \left( 3 + x_2^{(k)} - x_3^{(k)} \right)
$$
$$
x_2^{(k+1)} = \frac{1}{3} \left( 9 + x_1^{(k)} + x_3^{(k)} \right)
$$
$$
x_3^{(k+1)} = \frac{1}{5} \left( -6 - x_1^{(k)} + x_2^{(k)} \right)
$$

#### Initial Guess
Let $x^{(0)} = [0, 0, 0]$.

#### Iterations
- **Iteration 1**:
  $$
  x_1^{(1)} = \frac{1}{4} \left( 3 + 0 - 0 \right) = 0.75, \quad
  x_2^{(1)} = \frac{1}{3} \left( 9 + 0 + 0 \right) = 3, \quad
  x_3^{(1)} = \frac{1}{5} \left( -6 - 0 + 0 \right) = -1.2
  $$
  $$
  x^{(1)} = [0.75, 3, -1.2]
  $$

- **Iteration 2**:
  $$
  x_1^{(2)} = \frac{1}{4} \left( 3 + 3 - (-1.2) \right) = 1.8, \quad
  x_2^{(2)} = \frac{1}{3} \left( 9 + 0.75 - 1.2 \right) = 2.85, \quad
  x_3^{(2)} = \frac{1}{5} \left( -6 - 0.75 + 3 \right) = -0.75
  $$
  $$
  x^{(2)} = [1.8, 2.85, -0.75]
  $$

---

### Gauss-Seidel Method

#### Iteration Formula
For Gauss-Seidel, the formulas are:
$$
x_1^{(k+1)} = \frac{1}{4} \left( 3 + x_2^{(k)} - x_3^{(k)} \right)
$$
$$
x_2^{(k+1)} = \frac{1}{3} \left( 9 + x_1^{(k+1)} + x_3^{(k)} \right)
$$
$$
x_3^{(k+1)} = \frac{1}{5} \left( -6 - x_1^{(k+1)} + x_2^{(k+1)} \right)
$$

#### Initial Guess
Let $x^{(0)} = [0, 0, 0]$.

#### Iterations
- **Iteration 1**:
  $$
  x_1^{(1)} = \frac{1}{4} \left( 3 + 0 - 0 \right) = 0.75, \quad
  x_2^{(1)} = \frac{1}{3} \left( 9 + 0.75 + 0 \right) = 3.25, \quad
  x_3^{(1)} = \frac{1}{5} \left( -6 - 0.75 + 3.25 \right) = -0.7
  $$
  $$
  x^{(1)} = [0.75, 3.25, -0.7]
  $$

- **Iteration 2**:
  $$
  x_1^{(2)} = \frac{1}{4} \left( 3 + 3.25 - (-0.7) \right) = 1.7375, \quad
  x_2^{(2)} = \frac{1}{3} \left( 9 + 1.7375 - 0.7 \right) = 3.3458, \quad
  x_3^{(2)} = \frac{1}{5} \left( -6 - 1.7375 + 3.3458 \right) = -0.8783
  $$
  $$
  x^{(2)} = [1.7375, 3.3458, -0.8783]
  $$

---

### Comparison

| **Iteration** | **Jacobi $(x_1, x_2, x_3)$**         | **Gauss-Seidel $(x_1, x_2, x_3)$**   |
|---------------|----------------------------------------|----------------------------------------|
| 1             | $[0.75, 3.0, -1.2]$                   | $[0.75, 3.25, -0.7]$                  |
| 2             | $[1.8, 2.85, -0.75]$                  | $[1.7375, 3.3458, -0.8783]$           |
| 3             | $[1.65, 3.0167, -0.99]$               | $[1.6341, 3.2534, -0.9437]$           |

---

### Observations
- **Jacobi Method**:
  - Converges more slowly because it updates all variables simultaneously using only the previous iterationâ€™s values.
- **Gauss-Seidel Method**:
  - Converges faster as it uses updated values within the same iteration.