## Singular Value Decomposition

Before getting into the SVD an important concept will be reiterated to help understand the SVD better. Recalling  a matrix can rotate and stretch a vector by the following matrices

$$
A = \begin{bmatrix}
cos(\theta) & -sin(\theta) \\
sin(\theta) & cost(\theta)
\end{bmatrix},
\begin{bmatrix}
\alpha & 0 \\
0 & \alpha
\end{bmatrix}
$$
This is important because matrix can be transformed into an orthogonal set of vecvtor through rotations through unit orthonal vectors $u_1,u_2,..$ and stretched by singular values $\sigma_1,\sigma_2,...$



The SVD is an orthogonal matrix reduction so the 2 norm and Frobenius norm are important. If A is a real m x n matrix then there are orthogonal matrices, U,V such that

$$
U^TAV=\sum = diag(\sigma_1...,\sigma_p)
$$
therefore 
$$
A=U\sum V^T = \sigma_i u_iv_i
$$
since for orthogonal matrices $Q^T=Q^{-1}$ so $Q^TQ=I$

where 
- $p=min(m,n)$
- $\sigma_i$ are the singular values of A, which make up $\sum$
- $U_i$ are the left singular vectors of A
- $V_i$ are the right singular vectors of A

**Every matrix A has a SVD. The singular values are uniquely determined if A is square and the singular values are distinct**



The singular values of a matrix are the square roots of the eigenvalues of a matrix $A^TA$

$$
\sigma_i=\sqrt{\lambda_i(A^TA)}
$$
 The singular values measure how much A stretches or compresses vectors along certain directions. 
 - 2 Norm: largest signular value 
 - Frobenius Norm: square root of the sum of squares of the singular value

#### Connection to eigenvalues

If we look at the form of the matrix we see that they the singular values are essentially an eigenvalue 

$$
Av = \sigma u =\lambda v
$$


## How to Find the SVD of a Matrix

$$
A=U\sum V^T
$$

1. Compute $A^TA$ and $AA^T$
2. Find singular values, $\sigma_i$ of $A^TA$. From largest to smallest they form $\sum$
3. Compute $V^T$
	1. Eigenvectors of $A^TA$ form V
	2. They eigenvectors must have a unit norm and be mutually orthogonal so it is necessary to normalize them to make sure that $v$ is a unit vector and not just a direction
	3. To Normalize this, divide each component of the eignenvector by the norm, $||v||$ 
4. Compute $U=\frac{1}{\sigma_i}AV_i$

## Projections

### **What Does It Mean to Project?**

Projection refers to breaking a vector into components that lie in specific subspaces:

1. **Projection Onto a Subspace**:
   - If you have a subspace $S$, the projection of a vector $x$ onto $S$ is the "shadow" or component of $x$ that lies in $S$.
   - Mathematically, a projection matrix $P$ satisfies:
     - $P^2 = P$ (idempotence: applying the projection twice has no further effect).
     - $P = P^T$ (symmetry: projection preserves the vector's orientation within the subspace).

2. **Orthogonal Projections**:
   - An orthogonal projection splits a vector into two components: one in the subspace and one orthogonal to it.
   - Example: If $x$ is projected onto the subspace $S$, the remainder (error vector) is perpendicular to $S$.

---

### **Projections in the Context of SVD**

The **SVD** of a matrix $A$ is:
$$
A = U \Sigma V^T
$$
Where:
- $U$: Describes the **output space** of $A$.
- $V$: Describes the **input space** of $A$.

To construct projections:
- $U$ and $V$ are split into two parts:
  $$
  U = [U_r \; |\; \tilde{U}_r], \quad V = [V_r \; |\; \tilde{V}_r]
  $$
  - $U_r$: Spans the **range of $A$** (column space).
  - $\tilde{U}_r$: Spans the **null space of $A^T$**.
  - $V_r$: Spans the **range of $A^T$** (row space).
  - $\tilde{V}_r$: Spans the **null space of $A$**.

#### **Projections Defined in SVD**
1. **Projection Onto the Range of $A$ (Column Space)**:
   The range of $A$ is spanned by $U_r$, and the projection onto this space is:
   $$
   P_{\text{Ran}(A)} = U_r U_r^T
   $$
   This isolates the part of a vector that lies in $\text{Ran}(A)$.

2. **Projection Onto the Null Space of $A$**:
   The null space of $A$ is spanned by $\tilde{V}_r$, and the projection onto this space is:
   $$
   P_{\text{Null}(A)} = \tilde{V}_r \tilde{V}_r^T
   $$

3. **Projection Onto the Range of $A^T$ (Row Space)**:
   The range of $A^T$ is spanned by $V_r$, and the projection onto this space is:
   $$
   P_{\text{Ran}(A^T)} = V_r V_r^T
   $$

4. **Projection Onto the Null Space of $A^T$**:
   The null space of $A^T$ is spanned by $\tilde{U}_r$, and the projection onto this space is:
   $$
   P_{\text{Null}(A^T)} = \tilde{U}_r \tilde{U}_r^T
   $$

---

### **Difference Between $U_r$ and $V_r$**

1. **$U_r$ (Range of $A$)**:
   - Represents the **column space** of $A$, the output of the transformation $Ax$.
   - Used for projections like $U_r U_r^T$, which isolate components in the column space.

2. **$V_r$ (Range of $A^T$)**:
   - Represents the **row space** of $A$, the domain or input space of $A$.
   - Used for projections like $V_r V_r^T$, which isolate components in the row space.

In simple terms:
- $U_r$: Describes the **outputs** of $A$ (what $A$ creates).
- $V_r$: Describes the **inputs** of $A$ (what $A$ works on).


## Eigenvalues and Eigenvectors


Eigenvalues and eigenvectors are core concepts in linear algebra that describe how a transformation, such as a matrix, acts on vectors.

---

## 1. What Are Eigenvalues and Eigenvectors?

### **Eigenvectors**:
- An eigenvector of a matrix $A$ is a vector $x$ such that:
  $$
  A x = \lambda x
  $$
  - Here, $x$ remains in the **same direction** after the transformation $A$, but it may be scaled.
  - Eigenvectors point to the **principal directions** in which the transformation acts.
  - When a matrix A is applied to it,  it can actually change its length or reverse its direction. Not to be confused by changing its direction

### **Eigenvalues**:
- The **eigenvalue** $\lambda$ associated with an eigenvector $x$ is the scalar that describes how much $x$ is scaled (stretched or compressed) by $A$.


---

## 2. Intuition for Eigenvalues and Eigenvectors

Imagine a transformation $A$ applied to a vector space:
- **Eigenvectors** are the "privileged" directions that remain unchanged in orientation (only their lengths may change).
- **Eigenvalues** tell you how much the eigenvectors are scaled.

#### Example:
- Stretching a rubber sheet: Certain lines stretch more than others. The directions of these lines are eigenvectors, and the amount they stretch is given by eigenvalues.

---

## 3. How Are Projections Related to Eigenvalues and Eigenvectors?

### **Projections**:
- A projection maps a vector onto a subspace. For example, projecting onto a line means finding the component of the vector that lies along that line.

### **Connection**:
- When working with symmetric matrices (e.g., covariance matrices), eigenvectors form an **orthogonal basis** for the vector space.
- Projections onto these eigenvectors decompose a vector into its components along independent directions.

### **Eigenvalues in Projections**:
- Eigenvalues measure the "importance" of each eigenvector in representing the transformation.
- For a symmetric matrix, projecting a vector $x$ onto an eigenvector $v_i$ isolates how much of $x$ lies along $v_i$, scaled by the eigenvalue $\lambda_i$:
  $$
  \text{Projection of } x = \lambda_i v_i
  $$

---

## 4. Applications of Eigenvalues and Projections

1. **Principal Component Analysis (PCA)**:
   - Eigenvalues and eigenvectors are used to find the principal directions of variance in high-dimensional data.
   - Projections onto these directions simplify the data while retaining most of the variance.

2. **Stability Analysis**:
   - In systems of differential equations, eigenvalues determine whether solutions grow, decay, or oscillate over time.

3. **Rank and Dimensionality**:
   - Eigenvalues reveal the rank of a matrix. Zero eigenvalues indicate dependencies between rows or columns.

4. **Optimization**:
   - Projections onto eigenvectors help isolate independent factors in optimization problems, simplifying computations.


---

## 6. Eigenvalues and Projections in Portfolios

In finance, eigenvalues and eigenvectors are critical for understanding the **risk structure** of portfolios through the covariance matrix of asset returns:

### **1. Covariance Matrix**:
- Represents how asset returns move together.
- Eigenvectors indicate the **directions of independent risk factors**.
- Eigenvalues show the **magnitude of variance** explained by each factor.

### **2. Portfolio Risk**:
- The largest eigenvalue represents the **dominant risk factor**, such as market-wide effects.
- Small eigenvalues correspond to **idiosyncratic risks** or diversification opportunities.

### **3. Projections in Portfolios**:
- By projecting portfolio returns onto eigenvectors, we can isolate contributions from specific risk factors.
- Eigenvalues quantify the importance of these factors.

---

## Applications of SVD

The SVD has many important features that are critical for engineering and finance. One particularly important features/ extensions are as follows

### Low Rank Approximation 

A common technique for reducing the size of a matrix is a lower rank approximation of a matrix. A low rank approximiation takes a matrix's SVD decomposition and only keeps/maintain the largest k values singular values in the matrix. This matrix contains information that represents a portion of the original matrix but in a reduced size. 

A rank-1 matrix is a matrix that is represented by the outer product of 2 vectors, $A=uv^T$ which results in an nxn matrix as opposed to the inner product which results in a scaler. The rows and columns are simply scaled multiples of the other rows. 

Therefore a Rank-k matrix is simply a matrix that can be written as the sum of k rank-one matrices and cannot be written as the sum of k-1 or fewer rank one matrices. 
	- largest linearly indepdents columns of A has size k

#### Reasons for Low Rank Approximations

- Compression: A low-rank approximation is a compressed version of the matrix with much few numbers. 
- De-noising- If the matrix is noisy version of the true signal, then low rank can actually be more informative then the original
- Matrix Completion - Fill in a matrix with default values. Then compute the best rank-k approximation of A. 

#### How to Fund Low rank Approximation

1. Compute the SVD of A
2. Keep only the top k right singular vectors: Set $V_k^T$ equal to the first k rows of $V^T$ (a k x n matrix)
3. Keep only the top k left singular vector : Set $U_k$ equal to the first k columns of U
4. Keep only the top k singular values: Set $\sum_k$ equal to the first k rows and columns of $\sum$
5. The rank k approximation is then 
$$
A_k=U_k \sum k V_k^T 
$$

