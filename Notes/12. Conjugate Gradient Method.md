
This is another iterative method that is very famous and used heavily. 

# Conjugate Gradient Method

## Purpose
- Used to solve $Ax = b$ where $A$ is:
  - **Symmetric**: $A^T = A$
  - **Positive Definite**: $x^T A x > 0$ for any non-zero $x$.

## When to Use
- Efficient for large, sparse systems of linear equations.
- If $A$ is not symmetric or positive definite, the **biconjugate gradient method** must be used instead.

## Key Concepts

### Quadratic Form
$$
f(x) = \frac{1}{2} x^T A x - b^T x + c
$$
- $x$: Vector of variables.
- $A$: Symmetric positive definite matrix.
- $b$: Linear coefficient vector.
- $c$: Constant scalar.

### Gradient
$$
\nabla f(x) = A x - b
$$
- Represents the direction of steepest ascent of the matrix if the matrix is symmetric, positive definite
- If it is not symmetric then 
 $$
\nabla f^{'}(x) = \frac{1}{2}A^{T}x+\frac{1}{2}Ax-b
$$

### Critical Point
- The critical point occurs when $\nabla f(x) = 0$, giving:
$$
x^* = A^{-1} b
$$

---

# Gradient: Rate of Change or Slope

## Definition
- The **gradient vector** $\nabla f(x)$ points in the direction of the steepest rate of increase of $f(x)$. It is the rate of change or slope wrt input variables or the vector that points in the direction of greatest rate of increase of that function.
- **Magnitude** of $\nabla f(x)$: Represents how steep the slope is in that direction.
	- When $\nabla f(x) =0$ it means we have a critical point.

### General Form
$$
\nabla f(x) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots
\end{bmatrix}
$$
## Example
For $f(x, y) = x^2 + y^2$:
$$
\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = (2x, 2y)
$$
- At $(1, 1)$, $\nabla f(x, y) = (2, 2)$. The function increases most rapidly in this direction.

---

### Method of Steepest Descent

##### Objective
Minimize $f(x)$ by iteratively moving in the direction of the negative gradient $-\nabla f(x)$.

##### Procedure

1. Start with an initial guess $x_0$. Start at a random $x_0$ and slide down the paraboloid to the bottom in step $x_1,x_2,...$
2. Choose the direction where f(x) is descreasing the most rapidly so where $f^{'}=b-Ax_i$
	1. The error is $e_i=x_i-x$
3. Compute the **residual** (error):
$$
r_i = b - A x_i
$$
   - $r_i$: Represents how far $x_i$ is from the correct solution.
4. Compute $\alpha$
$$

\alpha = \frac{r_i^T r_i}{d_i^T A d_i}
$$
5. . Update the solution:
$$
x_{i+1} = x_i + \alpha r_i
$$
   - $\alpha$: Step size
   - $r_i = -f(x_i)=Ae_i$: Direction of steepest descent/ residual being the error transformed by A onto the space of b.

###### Finding Step Size $\alpha$
To minimize $f(x)$ along $d_i$:
$$
\alpha = \frac{r_i^T r_i}{r_i^T A r_i}
$$

#### Line Search in Steepest Descent

##### First Step

When choosing the first step where $x_1=x_0+\alpha r_0$, the line search process chooses the $\alpha$ which minimizes f for the following process.

###### Process
- Start with $x_0$ and perform a **line search** to choose $\alpha$ such that:
$$
x_1 = x_0 + \alpha d_0
$$

###### Condition for $\alpha$
- $\alpha$ minimizes $f(x)$ when:
$$
\frac{d}{d \alpha} f(x_1) = 0
$$
- Expanding this:
$$
\frac{d}{d \alpha} f(x_1) = f^{'}(x_1) r_0 = 0
$$

 If we note that $f^{'}(x_1)^T=-r_1^{T}$ then we see that the orthogonality condition means that $r_1^{T}r_0=0$ so we can replace our known values of both values which will give us the $\alpha$ we got above.

This means that we are choosing an alpha such that $r_0$ and $f^{'}(x_1)$ are orthogonal. The reason that whey would be orthogonal at the minimum is due to the face that the shape of this is a paraboloid. This means that the slope of the parabola at any point is the magnitude of the projection of the gradient onto the line. A projection is the rate of increase of f of the gradient onto the line. 


- The steepest descent method zigzags due to the nature of the gradient direction, all directions must be orthogonal to each other. This behavior slows convergence for ill-conditioned matrices.
- Steepest descent is less efficient for poorly conditioned problems (i.e., where $A$ has a large condition number).

---

## Convergence of Steepest Descent

$$
e_{i+1} = e_i + \frac{r_i^T r_i}{r_i^T A r_i} r_i
$$

- **So Steepest Descent is very dependent on** $A$, more specifically the eigenvalues of A.
### For a Symmetric Matrix $A$
- The eigenvalues of $A$ can be expressed as:
$$
A = V \Lambda V^T
$$
  - $V$: An orthonormal eigenvector matrix.
  - $\Lambda$: A diagonal matrix of eigenvalues.
### Error Terms
The error $e_i$ can be expressed as:
$$
e_i = \sum \xi_i v_i
$$
  - $v_i$: Eigenvectors of $A$.
  - $\xi_i$: Coefficients representing the projection of $e_i$ onto $v_i$.

### Convergence Behavior
- If $A$ has a **high condition number** (ratio of largest to smallest eigenvalue):
  - **Steepest Descent struggles to converge.**
- If eigenvalues are close:
  - **Convergence is efficient.**

### Special Cases
- If all eigenvalues are equal,:
  - **Steepest Descent converges after 1 step.**
- Convergence **slows as the condition number increases**.

---

# Conjugate Directions

- **If we somehow already know the answer**, in $n$ steps, we can arrive at the solution.

- **Steps**:
  - Pick a set of orthogonal directions $d_0, d_1, d_2, \dots$, and take one step in each direction so it lines up with $x$:
    $$
    x_i = x_{i-1} + \alpha_i d_i
    $$

- **Step Size** ($\alpha_i$):
  $$
  \alpha_i = \frac{r_i^T r_i}{d_i^T A d_i}
  $$

- **Search Direction**:
  - $d_i$ is just a **linear combination of previous residuals and $Ad_i$**.

This is now A orthogonal instead of just orthogonal

---

## Orthogonality and $A$-Orthogonality

- **Orthogonality**:
  - Directions $d_i$ and $d_j$ are orthogonal if:
    $$
    d_i^T d_j = 0 \quad \text{for } i \neq j.
    $$

- **$A$-Orthogonality (Conjugacy)**:
  - Directions $d_i$ and $d_j$ are $A$-orthogonal if:
    $$
    d_i^T A d_j = 0 \quad \text{for } i \neq j.
    $$

- **Relationship**:
  - $A$-orthogonality generalizes the idea of orthogonality by incorporating the matrix $A$.
  - $A$-orthogonality ensures the search directions remain independent and efficient in the context of minimizing $f(x) = \frac{1}{2} x^T A x - b^T x + c$.

---

## Subspace Construction

- The subspace $D_i$ is formed by the union of previous search directions:
  $$
  D_i = \text{span}(d_0, d_1, \dots, d_{i-1}).
  $$

- **Krylov Subspace**:
  - This is a Krylov subspace, created by repeatedly applying the matrix $A$ to the vector $b$:
    $$
    \{ b, Ab, A^2b, \dots, A^{n-1}b \}.
    $$

---

## Preconditioning

- **Purpose**:
  - A technique to improve the **condition number** of a matrix.

- **Process**:
  - If $M$ is a symmetric positive definite matrix that approximates $A$, solve:
    $$
    M^{-1} A x = M^{-1} b
    $$

- **Goal**:
  - Ensure eigenvalues are closely clustered, reducing the condition number and improving convergence speed.

---

## Conjugate Gradients

- **Key Idea**:
  - Set the search directions by the **conjugation** of the residuals:
    $$
    d_0 = r_0
    $$
    $$
    d_{i+1} = r_{i+1} + \beta_i d_i
    $$

- **Residuals**:
  - Residual $r_i$ is always orthogonal to previous search directions so each search directional is linearly independent:
    $$
    r_i^T d_j = 0 \quad \text{for } j < i.
    $$

- **Orthogonality**:
  - Residuals are also orthogonal to each other:
    $$
    r_i^T r_j = 0 \quad \text{for } i \neq j.
    $$

The residuals are just a linear comboination of the previous residual $r_i$ and $Ad_{i-1}$. $d_{i-1}$ means that each new $D_{i+1}$ is formed by the union of the previous subspace $D_i$ and the subspace $AD_i$ so the entire span is 

$$
D_{i}=span({d_0,Ad_0,A^2d_o})
$$
which is a **Krylov Subspace**, a subspace created by repeadelty applying a matrix to a vector. 

Some Insights:
- $AD_i$ is  $D_{i+1}$ and $r_{i+1}$ is orthogonal to $D_{i+1}$ so $r_{i+1}$ is orthogonal to $D_{i}$. 
- Most of the old search vectors aren't needed to ensure A orthogonality of new search vectors so it is efficient in time and space complexity.


---

## Key Equations for Conjugate Gradients

0. Initial Direction

$$
d_0=r_0=b_0-Ax_0
$$
2. **Step Size**:
   $$
   \alpha_i = \frac{r_i^T r_i}{d_i^T A d_i}
   $$

2. **Residual Update**:
   $$
   r_{i+1} = r_i - \alpha_i A d_i
   $$

3. **Direction Update**:
   $$
   \beta_{i+1} = \frac{r_{i+1}^T r_{i+1}}{r_i^T r_i}
   $$
   $$
   d_{i+1} = r_{i+1} + \beta_i d_i
   $$

---

## Steepest Descent vs. Conjugate Gradient

- **Steepest Descent**:
  - Takes steps directly along the negative gradient:
    $$
    x_{i+1} = x_i + \alpha_i (-\nabla f(x_i)).
    $$

- **Conjugate Gradient**:
  - More efficient, as it uses $A$-orthogonality to ensure search directions remain linearly independent, avoiding redundant steps.

- **Residual Relationship**:
  - For steepest descent:
    $$
    r_{i+1} = r_i - \alpha_i A d_i
    $$
  - For conjugate gradient ($d_{i+1}$ in previous section)
    $$
    r_{i+1} = r_i - \alpha_i A d_i + \beta_i d_i.
    $$
    